{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW 1.3",
      "provenance": [],
      "authorship_tag": "ABX9TyODj2anu1Zuz0QJU3AKnzSj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronyu888/mat-494-notebooks/blob/main/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te1qp7LWvM8V"
      },
      "source": [
        "# Chapter 1.3 Linear Regression\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj0YbBesvf4A"
      },
      "source": [
        "# 1.3.1 QR Decomposition\n",
        "QR factorization takes the general form $A = QR$ where $Q$ is the product of the Gram-Schmidt process, and $R$ is an upper triangular matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBZofZV312JS"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHrfDPe7DizX",
        "outputId": "75f79f8a-accc-491b-9ddc-a7ab2a45e372"
      },
      "source": [
        "A = np.random.randint(10, size = (3, 3))\n",
        "q, r = np.linalg.qr(A)\n",
        "print(A, '\\n')\n",
        "print(q, '\\n')\n",
        "print(r, '\\n')"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 1 6]\n",
            " [3 1 3]\n",
            " [2 3 1]] \n",
            "\n",
            "[[-0.48507125 -0.14927036 -0.86164044]\n",
            " [-0.72760688 -0.47766515  0.49236596]\n",
            " [-0.48507125  0.86576808  0.12309149]] \n",
            "\n",
            "[[-4.12310563 -2.66789188 -5.57831938]\n",
            " [ 0.          1.97036873 -1.46284951]\n",
            " [ 0.          0.         -3.56965324]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6_ROJpOEtVo",
        "outputId": "0f14b70a-c706-404a-a66c-407adb58c967"
      },
      "source": [
        "p = (np.matmul(q, r))\n",
        "print(p, '\\n')\n",
        "print(A)\n"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2. 1. 6.]\n",
            " [3. 1. 3.]\n",
            " [2. 3. 1.]] \n",
            "\n",
            "[[2 1 6]\n",
            " [3 1 3]\n",
            " [2 3 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsEl1R6VGxPt"
      },
      "source": [
        "As you can see from the above code, $Q$ dotted with $R$ gives us back $A$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzwrMTu2G-2m"
      },
      "source": [
        "# 1.3.2 Least-squares Problems\n",
        "We are trying to solve for the system $Ax = b$ where $A$ is an $n$ x $m$ matrix and $n > m$. If $n = m$, then we could just find the matrix inverse.\n",
        "Instead, we find an $Ax$ such that we minimize $\\|Ax-b\\|$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ofhHJ6dSImK",
        "outputId": "7e0f516b-0d53-4bec-de74-96be8e4f13ef"
      },
      "source": [
        "b = np.random.randint(10, size = (3, 1))\n",
        "x = np.matmul(np.matmul(np.linalg.inv(r),np.transpose(q)), b)\n",
        "Ax = np.matmul(A, x)\n",
        "print('solution vector: \\n', x, '\\n')\n",
        "print('b: \\n', b, '\\n')\n",
        "print('Ax: \\n', Ax, '\\n')\n"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "solution vector: \n",
            " [[-0.55172414]\n",
            " [ 1.20689655]\n",
            " [ 1.48275862]] \n",
            "\n",
            "b: \n",
            " [[9]\n",
            " [4]\n",
            " [4]] \n",
            "\n",
            "Ax: \n",
            " [[9.]\n",
            " [4.]\n",
            " [4.]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXWJovR4WRyd"
      },
      "source": [
        "As you can see from the above code, we generate a random $b$ vector and use the random $A, Q, R$ values from the previous QR decomposition example. $Ax$ should give a close approximation to $b$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAkUfaY2XQKs"
      },
      "source": [
        "# 1.3.3 Linear Regression\n",
        "Linear regression seeks to find an affine function to fit a data set as closely as possible. This is a minimization problem and when looked at in matrix form, is the exact same as the least-squares problem. "
      ]
    }
  ]
}
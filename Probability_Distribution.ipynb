{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Probability_Distribution.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOSbVlPRNwOqspjdZ9yoebD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronyu888/mat-494-notebooks/blob/main/Probability_Distribution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNGEc03apip-"
      },
      "source": [
        "#2.2 Probability Distribution\n",
        "A probability distribution is a function that gives the probabilities of different possible outcomes of an experiment.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD9Cup6LrmHZ"
      },
      "source": [
        "#2.2.1 Probability Axioms\n",
        "An **experiment** is any activity or process whose outcome is subject to uncertainty.\n",
        "\n",
        "The **sample space** $S$ of an experiment is the set of all possible outcomes of that experiment. It is usually more meaningful to study collections of outcomes from $S$ than individual outcomes.\n",
        "\n",
        "An **event** is any subset of outcomes contained in the sample space $S$. An event is simple if it consists of only one outcome and compound if it consists of multiple.\n",
        "\n",
        "The **probability distribution** is a function which assigns to each event $A$ a number $P(A)$ which will give a precise measure of the chance that $A$ will occur.\n",
        "\n",
        "*   $1 \\geq P(A) \\geq 0$\n",
        "*   $P(S) = 1$\n",
        "*   If $A_1,A_2,...$ is an infinite collection of disjoint events, then $P(A_1\\cup A_2\\cup ...) = \\sum\\limits_{i=1}^{\\infty} P(A_i)$\n",
        "*   For any event $A$, $P(A) + P(A') = 1$, from which $P(A) = 1 - P(A')$\n",
        "*   When events $A$ and $B$ are mutually exclusive, $P(A\\cup B) = P(A) + P(B)$\n",
        "*   For any two events $A$ and $B$, $P(A\\cup B) = P(A) + P(B) - P(A\\cap B)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU1bKOLCvkwG",
        "outputId": "0dd6f61b-fa03-450d-8c00-fe0e9832ee99"
      },
      "source": [
        "import random\n",
        "one, two, three, four, five = 0, 0, 0, 0, 0\n",
        "for i in range(10000):\n",
        "  num = random.randint(1, 5)\n",
        "  if num == 1:\n",
        "    one += 1\n",
        "  elif num == 2:\n",
        "    two += 1\n",
        "  elif num == 3:\n",
        "    three += 1\n",
        "  elif num == 4:\n",
        "    four += 1\n",
        "  else:\n",
        "    five += 1\n",
        "print(\"number of 1s:\", one, \"\\nnumber of 2s: \", two, \"\\nnumber of 3s: \", three, \"\\nnumber of 4s: \", four, \"\\nnumber of 5s: \", five)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of 1s: 2051 \n",
            "number of 2s:  1980 \n",
            "number of 3s:  1939 \n",
            "number of 4s:  2055 \n",
            "number of 5s:  1975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPlZQuJhxnCC"
      },
      "source": [
        "Given a random number generator with 5 equally likely outcomes, each outcome should happen around 2,000 times if run 10,000 times. From the code above, we can see that the probability is roughly evenly distributed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFAGSKPM21_v"
      },
      "source": [
        "#2.2.2 Conditional Probability\n",
        "**Condition probability** is defined as the likelihood of an event or outcome happening based on the occurrence of a previous event or outcome. It's expressed as a ratio of unconditional probabilities: the numerator is the probability of the intersection of the two events, whereas the denominator is the probability of the conditioning event $B$. The conditional probability of $A$ given $B$ is proportional to $P(A\\cap B)$.\n",
        "\n",
        "The conditional probability of $A$ given that $B$ has occurred is defined by $P(A|B) = \\frac{P(A\\cap B)}{P(B)}$\n",
        "\n",
        "Conditional probability also gives rise to the multiplication rule\n",
        "$P(A\\cap B) = P(A|B) \\cdot P(B)$. This is important because $P(A\\cap B)$ is often desired, and $P(B)$ and $P(A|B)$ are specified in the problem.\n",
        "\n",
        "$A$ and $B$ are independent events if $P(A|B) = P(A)$ or $P(A\\cap B) = P(A)\\cdot P(B)$. This also applies to collections of events as well.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IDi3SxhCL5k",
        "outputId": "23870f16-941b-41ab-965f-ec8a93d10970"
      },
      "source": [
        "A = 0.4\n",
        "B = 0.8\n",
        "cond = \"{:.2f}\".format(A * B / B)\n",
        "print(cond)\n",
        "A *= B\n",
        "cond = \"{:.2f}\".format(A * B / B)\n",
        "print(cond)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.40\n",
            "0.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-vAa3aPCd5l"
      },
      "source": [
        "From the above code, we can see that when we set $P(A)$ as a constant, the formula $P(A|B) = \\frac{P(A\\cap B)}{P(B)}$ equals $P(A)$. This verifies that $A$ is independent. When we make $A$ a function of $B$, $P(A|B) = 0.32$, verifying that $A$ is dependent on $B$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPaN2QzhC8Vm"
      },
      "source": [
        "#2.2.3 Discrete Random Variables\n",
        "A **discrete random variable** is a random variable whose possible values either constitute a finite set or else can be listed in an infinite sequence.\n",
        "\n",
        "A **probability mass function** of a discrete random variable is defined for every number $x$ by $p(x) = P(X = x) = P($all $ s \\in S:X(s) = x)$.\n",
        "\n",
        "The **cumulative distribution function** of a discrete random variable $X$ with pmf $p(x)$ is defined for every number $x$ by $F(x) = P(X \\leq x) = \\sum\\limits_{y:y\\leq x} p(y)$.\n",
        "\n",
        "Any random variable whose only possible values are 0 and 1 is called a **Bernoulli random variable**.\n",
        "\n",
        "The **Poisson distribution** is a discrete probability distribution that describes the probability of a given number of events occuring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event. A discrete random variable $X$ has a Poisson distribution with parameter $\\mu$ if the pmf of $X$ has the form $p(x;\\mu) = \\frac{e^{-\\mu}\\mu^x}{x!}$.\n",
        "\n",
        "The **expected value** of a random variable $X$ is a generalization of the weighted average and is intuitively the arithmetic mean of a large number of independent realizations of $X$. It can be written as $E(X) = \\mu_x = \\sum\\limits_{x\\in D} x\\cdot p(x)$.\n",
        "\n",
        "If the random variable $X$ has a set of possible values $D$ and pmf $p(x)$, then the expected value of any function $h(X)$, denoted by $E[h(x)]$ or $\\mu_{h(X)}$ is computed by $E[h(X)] = \\sum\\limits_D h(x)\\cdot p(x)$. In particular, $E(aX + b) = a \\cdot E(X) + b$.\n",
        "\n",
        "Let $X$ have pmf $p(x)$ and expected value $\\mu$. Then the **variance** of $X$, denoted by $V(x)$ or $\\sigma_X^2$ or just $\\sigma^2$ is $V(x) = \\sum\\limits_D (x-\\mu)^2 \\cdot p(x) = E[(X - \\mu)^2]$.\n",
        "\n",
        "The **standard deviation** of $X$ is $\\sigma_X = \\sqrt{\\sigma_X^2}$.\n",
        "\n",
        "If $X$ is a binomial random variable with parameters $n, p$, then $E(X) = np, V(X) = np(1-p), \\sigma_x = \\sqrt{np(1-p)}$.\n",
        "\n",
        "If $X$ is a Poisson distribution with parameter $\\mu$, then $E(X) = \\mu, V(X) = \\mu$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YbZHO6aW8Q0"
      },
      "source": [
        "#2.2.4 Continous Random Variables\n",
        "A random variable $X$ is **continuous** if possible values comprise either a single interval on the number line or a union of disjoint intervals.\n",
        "\n",
        "A **probability density function** of a continuous random variable $X$ is a function $f(x)$ such that for any two numbers $a$ and $b$ with $a\\leq b$, $P(a \\leq X \\leq b) = \\int_a^b f(x)dx$.\n",
        "\n",
        "The expected value of a continuous random variable $X$ with pdf $f(x)$ is $\\mu_X = E(X) = \\int_{-\\infty}^{\\infty} x\\cdot f(x)dx$.\n",
        "\n",
        "The variance of a continuous random variable $X$ with pdf $f(x)$ and the variance $\\mu$ is $\\sigma_X^2 = V(X) = \\int_{-\\infty}^{\\infty} (x-\\mu)^2 \\cdot f(x)dx = E[(X - \\mu)^2]$.\n",
        "\n",
        "The **standard deviation** of $X$ is $\\sigma_X = \\sqrt{V(X)}$.\n",
        "\n",
        "The expected value of an exponentially distributed random variable $X$ is $E(X) = \\int_0^\\infty x\\lambda e^{-\\lambda x}dx$.\n",
        "\n",
        "A continuous random variable $X$ is said to have a normal distribution with parameters $\\mu$ and $\\sigma$, where $-\\infty<\\mu<\\infty$ and $0<\\sigma$, if the pdf of $X$ is $f(x;\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-(x-\\mu)^2/(2\\sigma^2)}$.\n",
        "\n",
        "The normal distribution with parameter values $\\mu = 0$ and $\\sigma = 1$ is called the standard normal distribution. A random variable having a standard normal distribution is called a standard normal random variable and will be denoted by $Z$. The pdf of $Z$ is $f(z;0,1) = \\frac{1}{sqrt{2\\pi}}e^{-z^2/2}$. The graph of $f(z;0,1)$ is called the standard normal curve."
      ]
    }
  ]
}
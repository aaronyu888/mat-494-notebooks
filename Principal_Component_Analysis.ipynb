{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Principal_Component_Analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPHY2qiLXj46I3W/SWxVBo6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronyu888/mat-494-notebooks/blob/main/Principal_Component_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG_n68reBlqK"
      },
      "source": [
        "# Principal Component Analysis\n",
        "Commonly used for dimensionality reduction, the point of PCA is to obtain lower-dimensional data while preserving as much of the data's variation as possible.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItUUFoHaSBtc"
      },
      "source": [
        "#1.4.1 Singular Value Decomposition\n",
        "Given an $m$ x $n$ matrix $A$, we can decompose $A$ into $U\\Sigma V^T$, where $U$ is an orthogonal $n$ x $n$ matrix in which $u_1 \\geq u_2 \\geq ... \\geq u_n$ in their ability to describe the variance in the column vectors of $A$. $\\Sigma$ is a nonnegative diagonal matrix also in hierarchical order. $V^T$ is an orthogonal $m$ x $m$ matrix which corresponds to the $U$ and $\\Sigma$ matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oY4yLlIaAQj",
        "outputId": "15060d46-7690-4d03-bf1c-6f50402bddcf"
      },
      "source": [
        "import numpy as np\n",
        "A = np.random.randint(10, size = (4, 3))\n",
        "u, s, vt = np.linalg.svd(A, full_matrices=True)\n",
        "print(\"U:\", u.shape, \"\\nS:\", s.shape, \"\\nVt:\", vt.shape, \"\\n\")\n",
        "print(\"U: \\n\", u, \"\\nS: \\n\", s, \"\\nVt: \\n\", vt, \"\\n\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "U: (4, 4) \n",
            "S: (3,) \n",
            "Vt: (3, 3) \n",
            "\n",
            "U: \n",
            " [[-0.55757891  0.71955096  0.07883374  0.40637103]\n",
            " [-0.32556318 -0.33588895  0.88353935 -0.02335466]\n",
            " [-0.4649109   0.1422234  -0.14004076 -0.86256533]\n",
            " [-0.60578233 -0.59092938 -0.43992277  0.30049659]] \n",
            "S: \n",
            " [19.42045828  7.30349291  4.5282217 ] \n",
            "Vt: \n",
            " [[-0.69228069 -0.57473897 -0.43637433]\n",
            " [ 0.32973243  0.28595543 -0.89972552]\n",
            " [-0.64189093  0.76674938  0.00845134]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N6o4RWwbJ-f"
      },
      "source": [
        "As you can see from the code above, from the 4 x 3 matrix, $U$ is a 4 x 4 matrix, $\\Sigma$ is a 3 dimension diagonal matrix, and $V^T$ is a 3 x 3 matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YhNb8hxcDRV"
      },
      "source": [
        "#1.4.2 Low-Rank Matrix Approximations\n",
        "An SVD matrix can be truncated at the $k$th term where $k < r$ where $r = col(A)$. From this, we can define $A_k$ as $\\|A-A_k\\|_2^2 = \\sigma_{k+1}^2$. Furthermore, we can say that $\\|A-A_k\\|_2^2 \\leq \\|A-B\\|_2$ for any matrix $B \\in \\mathbb{R}^{nxm}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOewVXzjjU_u"
      },
      "source": [
        "#1.4.3 Principal Component Analysis\n",
        "Given a $p$ x $N$ matrix $X$, the sample mean of $X$ is given by $M = \\frac{1}{N}(X_1 +...+X_N)$. For $k = 1,...,N$, let $\\hat{X_k}=X_k-M$, then $B=[\\hat{X_1},\\hat{X_2},...,\\hat{X_N}]$. $B$ is is mean-deviation form and has a zero sample mean. The sample covariance matrix is the $p$ x $p$ matrix $S$ defined as $S = \\frac{1}{N - 1}BB^T$.\n",
        "\n",
        "The optimal choice of the first $k$ eigenvectors of $XX^T$ corresponding to the first $k$ largest eigenvalues is also the first $k$ columns of $V = [v_1,...,v_p]$ of the covariance matrix $XX^T$. The first principal compoent is the eigenvector corresponding to the largest eigenvalue of $XX^T$ and so on.\n",
        "\n",
        "The total variance of the data is the sum of the variances on the diagonal of S. The fraction of the variances of the first $k$ term truncation is $\\frac{\\Sigma_1^k\\lambda_j}{\\Sigma_1^p\\lambda_j}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ubRDYAtp4ko",
        "outputId": "faf31ec7-d4a9-4d14-d8d8-3b37d28e1455"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "X = np.random.randint(10, size=(4,3))\n",
        "pca = PCA(n_components = 3, svd_solver= 'full')\n",
        "pca.fit(X)\n",
        "PCA(n_components = 3)\n",
        "print(pca.explained_variance_ratio_)\n",
        "print(pca.singular_values_)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.57914783 0.35527874 0.06557343]\n",
            "[5.93154541 4.64577049 1.99589219]\n"
          ]
        }
      ]
    }
  ]
}
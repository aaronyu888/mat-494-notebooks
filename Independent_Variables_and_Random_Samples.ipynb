{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Independent_Variables_and_Random_Samples.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNWpWc/yEjGKnmDYuEfuH/Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronyu888/mat-494-notebooks/blob/main/Independent_Variables_and_Random_Samples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHGQIBhJ28H-"
      },
      "source": [
        "#Independent Variables and Random Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4VpxJwl3C7b"
      },
      "source": [
        "#2.3.1 Joint Probability Distributions\n",
        "A **joint probability distribution** shows a probability distribution for two or more random variables. The pmf is $p(x,y) = P(X = x$ and $Y = y)$.\n",
        "A **joint continuous distribution** is the continuous analogue of a joint discrete distribution. The probability that a pair of continuous random variables falls in a two-dimensional set  $A$ is obtained by integrating a function called the joint density function.\n",
        "\n",
        "For any two-dimensional set $A$, the joint probability density function $f(x,y)$ is $P[(X,Y) \\in A = \\int\\int_A f(x,y)dxdy$. In particular, if $A$ is the two-dimensional rectangle $\\{(x,y):a\\leq x\\leq b, c\\leq y\\leq d\\}$ then $P[(X,Y) \\in A] = P(a \\leq X \\leq b, c \\leq Y \\leq d) = \\int_a^b\\int_c^d f(x,y)dydx$.\n",
        "\n",
        "The marginal probability density functions of $X$ and $Y$ are given by $f_X(x) = \\int_{-\\infty}^\\infty f(x,y)dy$ and $f_Y(y) = \\int_{-\\infty}^\\infty f(x,y)dx$ for $-\\infty < x = y < \\infty$.\n",
        "\n",
        "Independent random variables describe a situation where the occurrence of one does not affect the probability of occurrence of the other. Two random variables $X$ and  $Y$ are independent if for every pair of $x$ and $y$ values $p(x,y) = p_X(x) \\cdot p_Y(y)$ when $X$ and $Y$ are discrete or $p(x,y) = f_X(x) \\cdot f_Y(y)$ when $X$ and $Y$ are continuous.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz5t9Eob7fb-"
      },
      "source": [
        "#2.3.2 Correlation and Dependence\n",
        "Correlations are useful because they can indicate a predictive relationship that can be exploited in practice. Covariance is a measure of the joint varaibility of two random variables. \n",
        "\n",
        "The covariance between two random variables $X$ and $Y$ is $Cov(X,Y) = E[(X-\\mu_X)(Y-\\mu_Y)] = \\sum_x\\sum_y (x-\\mu_X)(y-\\mu_Y)p(x,y)$ for discrete, $\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_X)(y - \\mu_Y)f(x,y)dxdy$ for continuous.\n",
        "\n",
        "The correlation coefficient of $X$ and $Y$ is defined by $\\rho_{X,Y}=\\frac{Cov(X,Y}{\\sigma_X\\cdot\\sigma_Y}$. If $X$ and $Y$ are independent, the $\\rho = 0$, but $\\rho = 0$ doesn't imply independence. $\\|\\rho\\|\\leq 1, \\rho = 1$ or $-1$ if $Y = aX + b$ for some numbers $a$ and $b$ with $a \\neq 0$.\n",
        "\n",
        " The correlation coefficient, when applied to a sample, is commonly represented by $r_{xy}$ and may be referred to as the sample Pearson correlation coefficient. Given paired data $\\{(x_1,y_1),...,(x_n,y_n)\\}$ consisting of $n$ pairs, $r_{xy} = \\frac {\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})} {\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}$ where the sample mean $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$ and analogously for $\\bar{y}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89aRggGMN29X"
      },
      "source": [
        "#2.3.3 Random Samples\n",
        "A simple random sample is a randomly selected subset of a population and often is used in practice. The random variableâ€™s $X_1,X_2,...,X_n$ are said to form a simple random sample of size $n$ if the $X_i$s are independent random variables and every $X_i$ has the same probability distribution.\n",
        "\n",
        "Let $X_1,X_2,...,X_n$ be a random sample from a distribution with mean value $\\mu$ and standard deviation $\\sigma$. Then $E(\\bar{X}) = \\mu_{\\bar{X}} = \\mu$, $V(\\bar{X}) = \\sigma_X^2=\\sigma^2/n$ and $\\sigma/\\sqrt{n}$. In addition, with $T_0 = X_1 + ... + X_n$, $E(T_0) = n\\mu$, $V(T_0)=n\\sigma^2$, and $\\sigma_{T_0} = \\sqrt{n\\sigma}$.\n",
        "\n",
        "The central limit theorem indicates that the properly normalized sum of independent random variables tends toward a normal distribution even if the original variables themselves are not normally distributed. This is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distrubtion can be applicable to many problems involving other types of distributions."
      ]
    }
  ]
}